# 텐서플로 동작 방식

# 계산 그래프의 연산
- 텐서 그래프란 일련의 연산 과정을 표현한 텐서플로우의 포맷임
- 연산을 위해서 입력값과 함수를 정의해야 하는데 아래와 같음
```
변수 = tf.placeholder(타입)
그래프 = tf.mul( 변수, 3.0 )
# y = x * 3
결과 = sess.run(그래프, feed_dict={변수: 3.0})
# 결과는 3 * 3 = 9 가 나올 것이다
```

# 다중 연산 중첩
- 여러 변수 연산(행렬)도 지원함
- 배열을 사용하여 값을 담고 텐서플로우에서 지원하는 행렬연산 함수를 사용함
```
변수 = tf.placeholder(타입, shape=행렬모양)
그래프 = tf.matmul( 변수, 정의된_행렬 )
```

# 다층 처리
- 단층 연산을 여러번 중첩한 다층 연산도 가능함
- 이전 연산 결과에 누적한다는 느낌으로 그래프에 누적하면 됨
```
첫번째_그래프 = tf.mul( 변수, 3.0 )
두번째_그래프 = tf.add( 첫번째_그래프 , 1)
...
x번째 결과 = sess.run(x번째_그래프, feed_dict{x번째까지 사용한 변수의 값...})
# 결과는 x번째 그래프까지의 결과가 나온다
# 여러 개의 결과를 동시에 얻을 수도 있다
# x, y 번째 결과 = sess.run([x번째, y번째 그래프], feed_dict={...})
```

# 비용 함수(손실 함수)
- 머신(정의된 모델)이 무엇을 기준으로 학습을 진행할 것인지를 정해야 함
- 평가 기준은 정답 문제로 (o, x) YES/NO로 가능하고 확률 문제로 (0 ~ 1) 값의 차이를 평가기준으로 정의할 수 있음
- 문제의 종류에 따라 더 적학합 평가 기준을 선택하는게 올바름
- 평가 기준을 `비용 함수의 값이 0으로 수렴`하는걸 올바르다고 볼 수 있음
- 비용 함수는 정의하기에 나름이고 자주 쓰이는 비용 함수를 살펴봄
    - L2 Norm = 제곱(정답 - 예측)
    - L1 Norm = 절대값(정답 - 예측)
    - cross-entropy = 정답과 예측의 확률적인 차이 값 ( 아래 그림에서는 확률이 0.8로 수렴해야 함)
        - ![](https://i.stack.imgur.com/CWK4C.png)
    - sigmoid CE, weighted CE, softmax CE, sparse softmax CE ...
- 비용 함수의 결과 용도는 특정 값에 수렴하냐(1) 또는 특정 확률에 수렴하냐(2)로 나뉨

# 역전파 구현
- 비용 함수를 사용해서 우리는 모델의 현재 상태를 수치화 할 수 있었음 (예측값과 원하는 값의 관계 정도)
- 위 수치를 모델이 더 적절해지도록(나아지도록) 업데이트하는데 역전파를 사용함
- 수학적인 정의가 있지만 개념만 보면 loss 값을 계산한 모델 함수를 (역미분)하여 loss값이 특정 방향으로 수렴하도록 거꾸로 반영하는 과정임
    - ![](https://t1.daumcdn.net/cfile/tistory/997C7A3359EEF5CA1F)

- 역전파도 정의하기 나름인데 대략 `학습률(learning_rate)`과 `최적화 방식(optimizaer)`으로 나뉘게 됨
    - optimizer 종류로 momentum, adagrad, adadelta ...

# 일괄 학습과 확률적 학습
- 여러 데이터를 모델에 돌려보고 결과 loss의 평균을 역전파 하는 합리적인 방법을 소개함
- batch라는 크기로 데이터를 나눠서 모델에 변수값으로 넣어줌
```
batch_size = 20
변수_세트 = tf.placeholder(모양=[None, 내가 원하는 최소 데이터 크기])
그래프 = tf.mat( 변수1개 일때 크기로 산정 , 그에 맞는 값 또는 행렬 크기)
# 중간 그래프 결과물은 항상 변수_세트의 크기만큼 포함 되어 있음
결과 = tf.session(그래프, feed_dict={변수_세트 : 변수_세트 * 최소 데이트 크기})
# 내가 batch크기를 20으로 했고 feed_dict로 20 크기의 batch를 넣었다면 결과로 20개가 나옴
```